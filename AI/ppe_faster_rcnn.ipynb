{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f79fa05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfbb4ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# print(len(os.listdir('annotations')))\n",
    "# print(len(os.listdir('images')))\n",
    "\n",
    "# !mkdir test_images\n",
    "# !mkdir test_annotations\n",
    "\n",
    "\n",
    "# random.seed(1234)\n",
    "# idx = random.sample(range(853), 170)\n",
    "\n",
    "# for img in np.array(sorted(os.listdir('images')))[idx]:\n",
    "#     shutil.move('images/'+img, 'test_images/'+img)\n",
    "\n",
    "# for annot in np.array(sorted(os.listdir('annotations')))[idx]:\n",
    "#     shutil.move('annotations/'+annot, 'test_annotations/'+annot)\n",
    "\n",
    "# print(len(os.listdir('annotations')))\n",
    "# print(len(os.listdir('images')))\n",
    "# print(len(os.listdir('test_annotations')))\n",
    "# print(len(os.listdir('test_images')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "034f1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a795639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_box(obj):\n",
    "    \n",
    "    xmin = float(obj.find('xmin').text)\n",
    "    ymin = float(obj.find('ymin').text)\n",
    "    xmax = float(obj.find('xmax').text)\n",
    "    ymax = float(obj.find('ymax').text)\n",
    "    \n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "adjust_label = 1\n",
    "\n",
    "def generate_label(obj):\n",
    "    if obj.find('name').text == 'W':\n",
    "        return 0\n",
    "\n",
    "    elif obj.find('name').text == \"WH\":\n",
    "        return 1\n",
    "    \n",
    "    elif obj.find('name').text == \"WHV\":\n",
    "        return 2\n",
    "    \n",
    "    elif obj.find('name').text == \"WV\":\n",
    "        return 3\n",
    "\n",
    "def generate_target(file): \n",
    "    with open(file) as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        objects = soup.find_all(\"object\")\n",
    "\n",
    "        num_objs = len(objects)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in objects:\n",
    "            boxes.append(generate_box(i))\n",
    "            labels.append(generate_label(i))\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32) \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64) \n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        \n",
    "        return target\n",
    "\n",
    "def plot_image_from_output(img, annotation):\n",
    "    \n",
    "    img = img.cpu().permute(1,2,0)\n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    for idx in range(len(annotation[\"boxes\"])):\n",
    "        xmin, ymin, xmax, ymax = annotation[\"boxes\"][idx].cpu()\n",
    "\n",
    "        if annotation['labels'][idx] == 1 :\n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "        \n",
    "        elif annotation['labels'][idx] == 2 :\n",
    "            \n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')\n",
    "            \n",
    "        else :\n",
    "        \n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='orange',facecolor='none')\n",
    "\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33802d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MaskDataset(object):\n",
    "    def __init__(self, transforms, path):\n",
    "        '''\n",
    "        path: path to train folder or test folder\n",
    "        '''\n",
    "        # transform module과 img path 경로를 정의\n",
    "        self.transforms = transforms\n",
    "        self.path = path\n",
    "        self.imgs = list(sorted(os.listdir(self.path)))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx): #special method\n",
    "        # load images ad masks\n",
    "        file_image = self.imgs[idx]\n",
    "        file_label = self.imgs[idx][:-3] + 'xml'\n",
    "        img_path = os.path.join(self.path, file_image)\n",
    "        \n",
    "        if 'test' in self.path:\n",
    "            label_path = f\"C:/Users/LGS/Desktop/STEP_7/ppe_det/faster_rcnn/worker-safety-1/test_anntations/{file_label}\"\n",
    "        else:\n",
    "            label_path = f\"C:/Users/LGS/Desktop/STEP_7/ppe_det/faster_rcnn/worker-safety-1/train_annotations/{file_label}\"\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        #Generate Label\n",
    "        target = generate_target(label_path)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.imgs)\n",
    "\n",
    "data_transform = transforms.Compose([  # transforms.Compose : list 내의 작업을 연달아 할 수 있게 호출하는 클래스\n",
    "        transforms.ToTensor() # ToTensor : numpy 이미지에서 torch 이미지로 변경\n",
    "    ])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset = MaskDataset(data_transform, 'worker-safety-1/train_images/')\n",
    "test_dataset = MaskDataset(data_transform, 'worker-safety-1/test_images/')\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, collate_fn=collate_fn)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bac8d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "  \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a09c701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LGS\\.conda\\envs\\test\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LGS\\.conda\\envs\\test\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model_instance_segmentation(4)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7993463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8db9f3bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------train start--------------------------\n",
      "epoch : 1, Loss : 17.276485443115234, time : 194.27614665031433\n",
      "epoch : 2, Loss : 11.476853370666504, time : 150.18771648406982\n",
      "epoch : 3, Loss : 9.484040260314941, time : 154.77302813529968\n",
      "epoch : 4, Loss : 8.941046714782715, time : 158.03654956817627\n",
      "epoch : 5, Loss : 8.269207000732422, time : 158.7150056362152\n",
      "epoch : 6, Loss : 8.0880708694458, time : 157.63674139976501\n",
      "epoch : 7, Loss : 7.602746963500977, time : 156.92012357711792\n",
      "epoch : 8, Loss : 7.300926208496094, time : 157.69312596321106\n",
      "epoch : 9, Loss : 6.7668280601501465, time : 158.93270087242126\n",
      "epoch : 10, Loss : 6.66456937789917, time : 159.4765181541443\n",
      "epoch : 11, Loss : 6.155863285064697, time : 158.34260630607605\n",
      "epoch : 12, Loss : 5.9710612297058105, time : 158.37443113327026\n",
      "epoch : 13, Loss : 5.614724159240723, time : 157.86891269683838\n",
      "epoch : 14, Loss : 5.324803352355957, time : 156.5039975643158\n",
      "epoch : 15, Loss : 5.137792110443115, time : 158.8550112247467\n",
      "epoch : 16, Loss : 5.088901996612549, time : 172.5471568107605\n",
      "epoch : 17, Loss : 4.985109329223633, time : 174.92028403282166\n",
      "epoch : 18, Loss : 5.060379981994629, time : 174.36028909683228\n",
      "epoch : 19, Loss : 4.567559242248535, time : 172.75646567344666\n",
      "epoch : 20, Loss : 4.348321914672852, time : 174.88866901397705\n",
      "epoch : 21, Loss : 4.161194801330566, time : 175.85515594482422\n",
      "epoch : 22, Loss : 4.050507068634033, time : 173.2815535068512\n",
      "epoch : 23, Loss : 4.197144508361816, time : 172.33233404159546\n",
      "epoch : 24, Loss : 3.998117446899414, time : 177.05455017089844\n",
      "epoch : 25, Loss : 3.985546112060547, time : 173.32028150558472\n",
      "epoch : 26, Loss : 3.83185076713562, time : 199.00263261795044\n",
      "epoch : 27, Loss : 3.8029274940490723, time : 193.96078538894653\n",
      "epoch : 28, Loss : 3.6911516189575195, time : 220.15421056747437\n",
      "epoch : 29, Loss : 3.6220059394836426, time : 219.93729090690613\n",
      "epoch : 30, Loss : 3.584498882293701, time : 204.91323447227478\n",
      "epoch : 31, Loss : 3.513150215148926, time : 204.57081532478333\n",
      "epoch : 32, Loss : 3.3621623516082764, time : 207.87722277641296\n",
      "epoch : 33, Loss : 3.562995433807373, time : 207.17136669158936\n",
      "epoch : 34, Loss : 3.372326135635376, time : 215.24905133247375\n",
      "epoch : 35, Loss : 3.493535280227661, time : 240.8821141719818\n",
      "epoch : 36, Loss : 3.215567111968994, time : 205.68959784507751\n",
      "epoch : 37, Loss : 3.4832961559295654, time : 208.7560715675354\n",
      "epoch : 38, Loss : 3.3215394020080566, time : 208.7285671234131\n",
      "epoch : 39, Loss : 3.3762142658233643, time : 192.98900771141052\n",
      "epoch : 40, Loss : 3.823556661605835, time : 225.26495337486267\n",
      "epoch : 41, Loss : 3.6163623332977295, time : 223.1697895526886\n",
      "epoch : 42, Loss : 4.015045166015625, time : 220.804594039917\n",
      "epoch : 43, Loss : 3.522862195968628, time : 213.3304479122162\n",
      "epoch : 44, Loss : 3.2353906631469727, time : 207.65457201004028\n",
      "epoch : 45, Loss : 3.114983558654785, time : 209.52947998046875\n",
      "epoch : 46, Loss : 3.2319300174713135, time : 219.8553068637848\n",
      "epoch : 47, Loss : 2.993126392364502, time : 232.63218760490417\n",
      "epoch : 48, Loss : 3.105201005935669, time : 216.6850073337555\n",
      "epoch : 49, Loss : 3.110762357711792, time : 207.84451031684875\n",
      "epoch : 50, Loss : 2.9893569946289062, time : 227.87779664993286\n",
      "epoch : 51, Loss : 2.8783960342407227, time : 216.96788835525513\n",
      "epoch : 52, Loss : 3.1247940063476562, time : 213.82456374168396\n",
      "epoch : 53, Loss : 2.8912558555603027, time : 218.11257410049438\n",
      "epoch : 54, Loss : 2.9393022060394287, time : 234.09497499465942\n",
      "epoch : 55, Loss : 3.1828854084014893, time : 215.09573245048523\n",
      "epoch : 56, Loss : 2.749876022338867, time : 203.4813587665558\n",
      "epoch : 57, Loss : 2.7217609882354736, time : 225.30524039268494\n",
      "epoch : 58, Loss : 2.6452577114105225, time : 211.20502877235413\n",
      "epoch : 59, Loss : 2.796980857849121, time : 227.89510989189148\n",
      "epoch : 60, Loss : 2.624181032180786, time : 213.24934148788452\n",
      "epoch : 61, Loss : 3.040437936782837, time : 205.89910244941711\n",
      "epoch : 62, Loss : 2.788809299468994, time : 223.20401692390442\n",
      "epoch : 63, Loss : 2.8456313610076904, time : 194.18558382987976\n",
      "epoch : 64, Loss : 2.384490489959717, time : 199.37560868263245\n",
      "epoch : 65, Loss : 2.3681156635284424, time : 217.89021229743958\n",
      "epoch : 66, Loss : 2.3305394649505615, time : 213.2291088104248\n",
      "epoch : 67, Loss : 2.3692102432250977, time : 222.436377286911\n",
      "epoch : 68, Loss : 2.59975266456604, time : 204.05327200889587\n",
      "epoch : 69, Loss : 2.8455817699432373, time : 196.03207993507385\n",
      "epoch : 70, Loss : 2.498163938522339, time : 197.258770942688\n",
      "epoch : 71, Loss : 2.1544270515441895, time : 210.37268328666687\n",
      "epoch : 72, Loss : 2.061197280883789, time : 218.38773393630981\n",
      "epoch : 73, Loss : 2.1212708950042725, time : 211.37032508850098\n",
      "epoch : 74, Loss : 2.148275375366211, time : 468.7581286430359\n",
      "epoch : 75, Loss : 2.285867214202881, time : 272.61174392700195\n",
      "epoch : 76, Loss : 2.3478920459747314, time : 551.8649663925171\n",
      "epoch : 77, Loss : 2.1803364753723145, time : 236.84441089630127\n",
      "epoch : 78, Loss : 1.9773533344268799, time : 578.0502047538757\n",
      "epoch : 79, Loss : 1.8640834093093872, time : 242.27610325813293\n",
      "epoch : 80, Loss : 1.814043402671814, time : 508.1952712535858\n",
      "epoch : 81, Loss : 1.9127041101455688, time : 228.2133777141571\n",
      "epoch : 82, Loss : 2.42146372795105, time : 220.5905704498291\n",
      "epoch : 83, Loss : 2.6404833793640137, time : 228.26515078544617\n",
      "epoch : 84, Loss : 2.2934954166412354, time : 223.47223043441772\n",
      "epoch : 85, Loss : 1.9824042320251465, time : 215.94096183776855\n",
      "epoch : 86, Loss : 1.9316660165786743, time : 222.78251719474792\n",
      "epoch : 87, Loss : 1.8147680759429932, time : 215.16957354545593\n",
      "epoch : 88, Loss : 1.9572770595550537, time : 219.97174715995789\n",
      "epoch : 89, Loss : 1.9215266704559326, time : 227.49304604530334\n",
      "epoch : 90, Loss : 2.196277141571045, time : 219.84187173843384\n",
      "epoch : 91, Loss : 2.1151421070098877, time : 220.47986149787903\n",
      "epoch : 92, Loss : 2.475041151046753, time : 220.8341944217682\n",
      "epoch : 93, Loss : 1.8939377069473267, time : 225.27857327461243\n",
      "epoch : 94, Loss : 2.188804864883423, time : 203.000648021698\n",
      "epoch : 95, Loss : 2.0303497314453125, time : 213.17213702201843\n",
      "epoch : 96, Loss : 1.943620204925537, time : 227.78121137619019\n",
      "epoch : 97, Loss : 1.7132258415222168, time : 217.28586888313293\n",
      "epoch : 98, Loss : 2.0624771118164062, time : 215.92288064956665\n",
      "epoch : 99, Loss : 1.7971680164337158, time : 214.29958772659302\n",
      "epoch : 100, Loss : 2.1406195163726807, time : 199.67093634605408\n"
     ]
    }
   ],
   "source": [
    "print('----------------------train start--------------------------')\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    i = 0    \n",
    "    epoch_loss = 0\n",
    "    for imgs_dum, annotations_dum in data_loader:\n",
    "        i += 1\n",
    "        imgs = []\n",
    "        annotations = []\n",
    "        for img, anno in zip(imgs_dum, annotations_dum):\n",
    "            if len(anno['boxes']) == 0:\n",
    "                continue\n",
    "            imgs.append(img.to(device))\n",
    "            annotations.append({k: v.to(device) for k, v in anno.items()})\n",
    "        loss_dict = model(imgs, annotations) \n",
    "        losses = sum(loss for loss in loss_dict.values())        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "        epoch_loss += losses\n",
    "    print(f'epoch : {epoch+1}, Loss : {epoch_loss}, time : {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23343945",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'model_{num_epochs}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44d4c25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'model_{num_epochs}.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04a57996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, img, threshold):\n",
    "    model.eval()\n",
    "    preds = model(img)\n",
    "    for id in range(len(preds)) :\n",
    "        idx_list = []\n",
    "\n",
    "        for idx, score in enumerate(preds[id]['scores']) :\n",
    "            if score > threshold : \n",
    "                idx_list.append(idx)\n",
    "\n",
    "        preds[id]['boxes'] = preds[id]['boxes'][idx_list]\n",
    "        preds[id]['labels'] = preds[id]['labels'][idx_list]\n",
    "        preds[id]['scores'] = preds[id]['scores'][idx_list]\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58c638d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import timeit\n",
    "\n",
    "model = model.to('cuda')\n",
    "model.eval() \n",
    "video_path = \"C:/Users/LGS/Desktop/STEP_7/ppe_det/data_set/test_worker.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('multiple_box.mp4', fourcc, cap.get(5), (int(cap.get(3)), int(cap.get(4))))\n",
    "with torch.no_grad(): \n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "            start_time = timeit.default_timer()\n",
    "            frame_tensor = frame\n",
    "#             frame_tensor = cv2.resize(frame, (640,640))\n",
    "            \n",
    "            results = model([data_transform(frame_tensor).to(device)])\n",
    "            boxes = results[0]['boxes'][results[0]['scores'] > 0.7]\n",
    "            boxes = boxes.cpu().numpy()\n",
    "\n",
    "            # 프레임에 사각형 그리기\n",
    "            for box, label, score in zip(boxes, results[0]['labels'], results[0]['scores']):\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 2)\n",
    "\n",
    "                # 레이블 정보와 스코어를 텍스트로 생성\n",
    "                if label == 0:\n",
    "                    label_text = f\" W {score:.2f}\"\n",
    "                if label == 1:\n",
    "                    label_text = f\" WH {score:.2f}\"\n",
    "                if label == 2:\n",
    "                    label_text = f\" WHV {score:.2f}\"\n",
    "                if label == 3:\n",
    "                    label_text = f\" WV {score:.2f}\"\n",
    "                # 텍스트를 프레임에 추가\n",
    "                cv2.putText(frame, label_text, (int(xmin), int(ymin) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "            terminate_time = timeit.default_timer()\n",
    "            fps = str(int(1. / (terminate_time - start_time)))\n",
    "\n",
    "            cv2.putText(frame, \"fps = {} \".format(fps), (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0))\n",
    "            cv2.imshow(\"YOLOv8 Inference\", frame)\n",
    "\n",
    "            out.write(frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "054d0c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
       "  'scores': tensor([], device='cuda:0', grad_fn=<IndexBackward0>)}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fdd0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_idx = 1\n",
    "print(\"Target : \", annotations[_idx]['labels'])\n",
    "plot_image_from_output(imgs[_idx], annotations[_idx])\n",
    "print(\"Prediction : \", pred[_idx]['labels'])\n",
    "plot_image_from_output(imgs[_idx], pred[_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea84951",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_idx = 1\n",
    "print(\"Target : \", annotations[_idx]['labels'])\n",
    "plot_image_from_output(imgs[_idx], annotations[_idx])\n",
    "print(\"Prediction : \", pred[_idx]['labels'])\n",
    "plot_image_from_output(imgs[_idx], pred[_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_from_output(img, annotation):\n",
    "    \n",
    "    img = img.cpu().permute(1,2,0)\n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    for idx in range(len(annotation[\"boxes\"])):\n",
    "        xmin, ymin, xmax, ymax = annotation[\"boxes\"][idx].cpu()\n",
    "\n",
    "        if annotation['labels'][idx] == 1 :\n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "        \n",
    "        elif annotation['labels'][idx] == 2 :\n",
    "            \n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')\n",
    "            \n",
    "        else :\n",
    "        \n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='orange',facecolor='none')\n",
    "\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931ead7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "labels = []\n",
    "preds_adj_all = []\n",
    "annot_all = []\n",
    "\n",
    "for im, annot in tqdm(test_data_loader, position = 0, leave = True):\n",
    "    im = list(img.to(device) for img in im)\n",
    "    #annot = [{k: v.to(device) for k, v in t.items()} for t in annot]\n",
    "\n",
    "    for t in annot:\n",
    "        labels += t['labels']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds_adj = make_prediction(model, im, 0.5)\n",
    "        preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]\n",
    "        preds_adj_all.append(preds_adj)\n",
    "        annot_all.append(annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224f2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Tutorial-Book-Utils/\n",
    "import utils_ObjectDetection as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_metrics = []\n",
    "for batch_i in range(len(preds_adj_all)):\n",
    "    sample_metrics += utils.get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5) \n",
    "\n",
    "true_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # 배치가 전부 합쳐짐\n",
    "precision, recall, AP, f1, ap_class = utils.ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels))\n",
    "mAP = torch.mean(AP)\n",
    "print(f'mAP : {mAP}')\n",
    "print(f'AP : {AP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e94df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "+"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
